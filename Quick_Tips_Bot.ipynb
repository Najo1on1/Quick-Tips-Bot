{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9696ed71606145f8a59b091fe3f10a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2bd6fb3522a844d9b790c877997d01c0",
              "IPY_MODEL_2fa40cabbed746529f8d23c057cb8e6a",
              "IPY_MODEL_d60493375f3a4b51b2fe1f297336ce3d"
            ],
            "layout": "IPY_MODEL_2d608077af7e432a8a7e9117ec3d2970"
          }
        },
        "2bd6fb3522a844d9b790c877997d01c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfdf8cd286c348a083ea28226f2a724a",
            "placeholder": "​",
            "style": "IPY_MODEL_c40d389529344fedaf78226e77c847bc",
            "value": "llama-2-13b-chat.ggmlv3.q5_K_M.bin: 100%"
          }
        },
        "2fa40cabbed746529f8d23c057cb8e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_105a1540c94f4d01a7e53571e32c72ae",
            "max": 9229634688,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d8c1d6f817b495991a702ad925117fb",
            "value": 9229634688
          }
        },
        "d60493375f3a4b51b2fe1f297336ce3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed9f1941d59423b8910b5fe77e0a33a",
            "placeholder": "​",
            "style": "IPY_MODEL_8b8d361f69104432a2da059245ac6650",
            "value": " 9.23G/9.23G [02:32&lt;00:00, 51.9MB/s]"
          }
        },
        "2d608077af7e432a8a7e9117ec3d2970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfdf8cd286c348a083ea28226f2a724a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c40d389529344fedaf78226e77c847bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "105a1540c94f4d01a7e53571e32c72ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8c1d6f817b495991a702ad925117fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bed9f1941d59423b8910b5fe77e0a33a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8d361f69104432a2da059245ac6650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3390e435046459bae6f9808d8805898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Your Question:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f079b44cbd004b96922948c612659a0f",
            "placeholder": "Type your question here...",
            "style": "IPY_MODEL_b329c1cb170f49878334dffc6560c85f",
            "value": "define a compass"
          }
        },
        "f079b44cbd004b96922948c612659a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b329c1cb170f49878334dffc6560c85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **A simple Bot powered by Llama**\n",
        "<center>\n",
        "\n",
        "[![link](https://i.imgur.com/itX91TX.png)](https://huggingface.co/meta-llama/Llama-2-13b-chat)\n",
        "Click on picture for more information\n",
        "</center>"
      ],
      "metadata": {
        "id": "4bKQIsIq-d8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases.\n",
        "\n",
        "It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety.\n",
        "\n",
        "`llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
        "\n",
        "`GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage."
      ],
      "metadata": {
        "id": "PnV5UC7A2vBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install All the Required Packages**\n",
        "In this section, we meticulously prepare the environment necessary for leveraging the advanced capabilities of LLaMA 2, a cutting-edge series of generative text models. This preparation involves installing Python bindings for LLaMA C++ and dependencies that facilitate the integration of these powerful models into our Colab environment. The section is geared towards ensuring that users can seamlessly interact with LLaMA models, specifically the 13B variant optimized for chat, by installing `llama-cpp-python` and `huggingface_hub`. These installations enable us to access and utilize the model hosted by 'TheBloke' on Hugging Face, a platform renowned for its extensive repository of machine learning models. Furthermore, we define `model_name_or_path` and `model_basename` to pinpoint the exact version of the model we intend to use, highlighting our focus on a model variant designed for chat applications and optimized through quantization for efficient performance. This setup phase is crucial for the successful execution of subsequent operations involving the model within our project."
      ],
      "metadata": {
        "id": "YQZBmz7I5neU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0avf7xx2lcj",
        "outputId": "9f43cde3-139c-49e8-ff6c-cb72a556b67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.6/1.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install the LLaMA C++ Python bindings and dependencies\n",
        "!pip install --quiet huggingface_hub\n",
        "!pip install --quiet llama-cpp-python==0.1.78"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version of LLaMA (2-13B) designed for chat, hosted by 'TheBloke' on a platform like Hugging Face.\n",
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "\n",
        "# Specify the basename of the model file. This is the filename under which the model is stored.\n",
        "# It indicates that the model uses a binary file format (.bin), which is common for storing\n",
        "# pre-trained neural network models. The name includes several details about the model:\n",
        "# - 'llama-2-13b-chat' suggests that this is a LLaMA model specifically tuned for chat applications.\n",
        "# - 'ggmlv3' likely refers to the version or format of the model.\n",
        "# - 'q5_K_M' might denote specific model parameters or quantization details,\n",
        "#   indicating how the model was compressed or optimized.\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_K_M.bin\""
      ],
      "metadata": {
        "id": "qJ90LnMv54Y-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import All the Required Libraries**\n",
        "In this section, we focus on setting up the essential Python libraries needed for our project, ensuring seamless interaction with the LLaMA 2 model and the broader Hugging Face ecosystem. By importing `hf_hub_download` from the `huggingface_hub` library, we gain the ability to effortlessly download model files and associated resources from the Hugging Face Model Hub, a central repository for machine learning models. This function streamlines accessing various models and their components, such as configuration files or weights, vital for our model's operation. Additionally, we import the `Llama` class from the `llama_cpp` module, a Python wrapper that provides a convenient interface to LLaMA model implementations in C++. This setup enables the execution of complex NLP tasks by leveraging the efficiency and performance optimizations inherent in the C++ implementations, all within a familiar Python environment."
      ],
      "metadata": {
        "id": "6lOmpKB36RJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the hf_hub_download function from the huggingface_hub library.\n",
        "# This function is used to download files from the Hugging Face Model Hub.\n",
        "# It is particularly useful for retrieving model files, configuration files, or any other\n",
        "# resources associated with models hosted on the Hugging Face Hub. The function requires\n",
        "# at least the name or path of the repository (model) on the Hub and the filename of the\n",
        "# resource to download. Additional parameters can specify versioning, caching behavior,\n",
        "# and more.\n",
        "\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "Ak3ZtGjM6Wdp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Llama class from the llama_cpp module.\n",
        "# The Llama class provides an interface to the LLaMA (Large Language Model - Meta AI) model implementations\n",
        "# in C++. This class enables the instantiation of a LLaMA model object in Python, allowing for the execution\n",
        "# of various NLP tasks such as text generation, question answering, and more, leveraging the efficiency\n",
        "# and performance optimizations of C++.\n",
        "# The llama_cpp module is a Python wrapper that facilitates interaction with the underlying C++ implementation\n",
        "# of LLaMA models, making it accessible and usable within Python environments without needing direct\n",
        "# C++ integration.\n",
        "\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "85XOzmui6rGN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download the Model**\n",
        "In this section, we employ the `hf_hub_download` function, a pivotal tool from the Hugging Face Hub library, to precisely obtain the specific model file necessary for our project. By specifying the repository identifier and filename, we ensure the accurate retrieval of the LLaMA 2 model variant optimized for chat applications. This process results in the function returning the local file path of the downloaded model, providing us with a direct link to the essential resource. This local path is critical, as it serves as the gateway through which we can load and interact with the model within our application. This step not only streamlines the setup phase but also solidifies the foundation for executing subsequent operations involving the model, facilitating a seamless transition from download to deployment in our analytical or generative tasks."
      ],
      "metadata": {
        "id": "haAb9kNm6J9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the hf_hub_download function to download the specific model file from the Hugging Face Hub.\n",
        "# The function returns the local file path to the downloaded file. This path can then be used to load\n",
        "# or further process the model file in your application.\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "9696ed71606145f8a59b091fe3f10a14",
            "2bd6fb3522a844d9b790c877997d01c0",
            "2fa40cabbed746529f8d23c057cb8e6a",
            "d60493375f3a4b51b2fe1f297336ce3d",
            "2d608077af7e432a8a7e9117ec3d2970",
            "bfdf8cd286c348a083ea28226f2a724a",
            "c40d389529344fedaf78226e77c847bc",
            "105a1540c94f4d01a7e53571e32c72ae",
            "0d8c1d6f817b495991a702ad925117fb",
            "bed9f1941d59423b8910b5fe77e0a33a",
            "8b8d361f69104432a2da059245ac6650"
          ]
        },
        "id": "qBgdGV4b6MxG",
        "outputId": "200ad673-43cc-4b12-96ea-0740140ba7bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-13b-chat.ggmlv3.q5_K_M.bin:   0%|          | 0.00/9.23G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9696ed71606145f8a59b091fe3f10a14"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the Model**\n",
        "In this section, the focus shifts to initializing the LLaMA model within our computational environment, a critical step that bridges the gap between acquiring the model and utilizing its capabilities. This process starts with verifying the presence of the model file, a precaution that ensures our operations proceed with the actual model at hand, preventing runtime errors related to file absence. Following this check, we attempt to load the model, employing error handling mechanisms to gracefully manage any issues that arise during this phase. The configuration parameters—number of threads, batch size, and GPU layers—are meticulously adjusted to align with our hardware's capabilities, optimizing for both performance and resource utilization. Additionally, a conditional check for CUDA availability underscores the adaptability of our setup, allowing for dynamic allocation between GPU and CPU resources based on the execution environment, ensuring that the model loading is as efficient and effective as possible."
      ],
      "metadata": {
        "id": "VQ6OYnI46kKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "model_path = model_path\n",
        "\n",
        "# Check if the model is already downloaded\n",
        "if not os.path.exists(model_path):\n",
        "    # Code to download the model goes here\n",
        "    pass  # Replace this with your download code\n",
        "\n",
        "# Attempt to load the model with error handling\n",
        "try:\n",
        "    lcpp_llm = None\n",
        "    # Assuming 'Llama' is your model class; adjust accordingly\n",
        "    lcpp_llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_threads=2,  # CPU cores\n",
        "        n_batch=512,  # Consider VRAM\n",
        "        n_gpu_layers=32  # Adjust based on your GPU and model\n",
        "    )\n",
        "    # Verify the number of GPU layers loaded\n",
        "    print(f\"Number of GPU layers loaded: {lcpp_llm.params.n_gpu_layers}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model: {e}\")\n",
        "\n",
        "# Optional: Check if CUDA is available for PyTorch and whether the model is on GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. Model is utilizing GPU.\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Model is on CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEtW61z0aBjn",
        "outputId": "3acd8f3b-fdfe-4cf8-9a2a-86866231e1e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPU layers loaded: 32\n",
            "CUDA is not available. Model is on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Create a Prompt Template**\n",
        "In this section, we meticulously craft a structured template to guide the interaction between the user and the LLaMA model, ensuring the generation of relevant and engaging responses. This begins with defining the user's inquiry, a specific question that prompts the model's engagement. To contextualize the model's response, a system prompt is introduced, portraying the model as a knowledgeable, digital encyclopedia capable of providing detailed and insightful answers. By combining these elements—system context and user query—into a cohesive prompt template, we establish a clear framework for the model's operation. This structured approach not only facilitates the elicitation of informative responses but also enhances the user experience by generating outputs that closely align with the user's expectations and the conversational tone set by the system prompt."
      ],
      "metadata": {
        "id": "iE-M307R6_pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the inquiry question that will be posed to the model. This variable contains the text of the question\n",
        "# that the user wants to ask.\n",
        "the_inquiry = \"what does a fun euro trip look like?\"\n",
        "\n",
        "# Define the system prompt, which sets the context for the model's response. This prompt informs the model\n",
        "# of its role, thus guiding the tone and type of response expected.\n",
        "system_prompt = \"\"\"\n",
        "SYSTEM: You are a digitalized encyclopedia with up-to-date knowledge about the world. You offer fun detailed answers to questions.\n",
        "\"\"\"\n",
        "\n",
        "# Construct the user prompt by embedding the user's inquiry within a formatted string that labels it as a user's question.\n",
        "user_prompt = f\"USER: {the_inquiry}\"\n",
        "\n",
        "# Combine the system prompt and user prompt into a single template, followed by a cue (\"Suggestion:\\n\") for the model\n",
        "prompt_template = f\"{system_prompt}\\n{user_prompt}\\n\\nSuggestion:\\n\""
      ],
      "metadata": {
        "id": "85XUjpWTbj2o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generating the Response**\n",
        "In this section, we leverage the capabilities of the LLaMA C++ language model to translate our carefully crafted prompt template into a coherent and contextually appropriate response. Through a precise invocation of the `lcpp_llm` function, we pass the assembled prompt along with a set of finely tuned parameters—max tokens, temperature, top_p, repeat penalty, and top_k—each meticulously chosen to influence the content, length, and creativity of the model's output. These parameters are instrumental in shaping the model's response, ensuring it not only adheres to the query's context but also maintains a balance between relevance and inventiveness. Following the generation process, the response is retrieved and displayed, providing insights or answers aligned with the user's initial inquiry. This step is crucial, as it embodies the interaction between the user's curiosity and the model's artificial intelligence, culminating in the delivery of a detailed, engaging response."
      ],
      "metadata": {
        "id": "aT8pg6zt7QzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a response using the LLaMA C++ language model (`lcpp_llm`). This function call\n",
        "# requests the model to process the constructed `prompt_template` and produce a response\n",
        "# adhering to the specified parameters. Each parameter controls a different aspect of the\n",
        "# generation process, influencing how the response is formulated.\n",
        "\n",
        "response = lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
        "                    repeat_penalty=1.2, top_k=150,\n",
        "                    echo=True)"
      ],
      "metadata": {
        "id": "d1HalDencLsQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVxvIr-XcaDR",
        "outputId": "ede8bf10-1307-4bfe-d4a6-0ef20347f6cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-7976138c-eb3a-43fe-aa3e-7ab8e4a96ddd', 'object': 'text_completion', 'created': 1712477238, 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_K_M.bin', 'choices': [{'text': \"\\nSYSTEM: You are a digitalized encyclopedia with up-to-date knowledge about the world. You offer fun detailed answers to questions.\\n\\nUSER: what does a fun euro trip look like?\\n\\nSuggestion:\\nA fun Euro trip can include exploring historic cities, visiting famous landmarks, enjoying local cuisine and drinks, attending cultural events, and taking in the natural beauty of Europe's diverse landscapes. Here are some specific ideas for a memorable Euro trip:\\n\\n1. Explore Rome's Colosseum and Vatican City, then indulge in delicious Italian cuisine like pizza and pasta.\\n2. Visit the iconic Eiffel Tower in Paris and sample French wines and cheeses at a charming bistro.\\n3. Take a gondola ride through Venice's canals and admire St. Mark's Square, then try some authentic seafood risotto for dinner.\\n4. Wander the medieval streets of Prague and visit Charles Bridge, followed by a night at a lively beer hall.\\n5. Enjoy Barcelona's beaches and modernist architecture, including Antoni Gaudí's famous Sagrada Família.\\n6. Take in the breathtaking scenery of Switzerland's Jungfrau region, with hiking trails, mountain trains, and picturesque\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 54, 'completion_tokens': 256, 'total_tokens': 310}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated response text from the LLaMA model.\n",
        "# After invoking the LLaMA model to generate a response based on the provided prompt and parameters,\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZXWM3uKcZ4T",
        "outputId": "40b4b314-3fcc-4cca-8a27-9def1b7961b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SYSTEM: You are a digitalized encyclopedia with up-to-date knowledge about the world. You offer fun detailed answers to questions.\n",
            "\n",
            "USER: what does a fun euro trip look like?\n",
            "\n",
            "Suggestion:\n",
            "A fun Euro trip can include exploring historic cities, visiting famous landmarks, enjoying local cuisine and drinks, attending cultural events, and taking in the natural beauty of Europe's diverse landscapes. Here are some specific ideas for a memorable Euro trip:\n",
            "\n",
            "1. Explore Rome's Colosseum and Vatican City, then indulge in delicious Italian cuisine like pizza and pasta.\n",
            "2. Visit the iconic Eiffel Tower in Paris and sample French wines and cheeses at a charming bistro.\n",
            "3. Take a gondola ride through Venice's canals and admire St. Mark's Square, then try some authentic seafood risotto for dinner.\n",
            "4. Wander the medieval streets of Prague and visit Charles Bridge, followed by a night at a lively beer hall.\n",
            "5. Enjoy Barcelona's beaches and modernist architecture, including Antoni Gaudí's famous Sagrada Família.\n",
            "6. Take in the breathtaking scenery of Switzerland's Jungfrau region, with hiking trails, mountain trains, and picturesque\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Greating the Chat Bot**\n",
        "In this section, we introduce 'Quick_Tips,' an interactive chatbot designed to serve as a digitalized encyclopedia, providing users with up-to-date, detailed answers on a wide array of topics. The core functionality of Quick_Tips is encapsulated in the `ask_question` function, which orchestrates the interaction process. Upon receiving a user's inquiry, the function constructs a prompt template, melding the query with a predefined system context to guide the model's response generation. Utilizing the LLaMA C++ language model, the chatbot processes this template, generating a response tailored to the user's question without revealing the underlying prompt mechanics, thereby maintaining a seamless conversational flow.\n",
        "\n",
        "To facilitate user interaction, an input box is presented, inviting users to submit their questions directly within the Colab environment. This setup not only democratizes access to information by leveraging the chatbot's encyclopedic knowledge but also enhances user engagement through real-time, personalized responses. The incorporation of event-driven programming allows Quick_Tips to handle user queries efficiently, displaying answers promptly and encouraging a dynamic exchange of information, reflective of the chatbot's aim to inform and assist users in their quest for knowledge."
      ],
      "metadata": {
        "id": "aB-dCZhdTYxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(question):\n",
        "    # Construct the prompt template\n",
        "    the_inquiry = question\n",
        "    system_prompt = \"SYSTEM: You are a digitalized encyclopedia with up-to-date knowledge about the world. You offer fun detailed answers to questions.\"\n",
        "    user_prompt = f\"USER: {the_inquiry}\"\n",
        "    prompt_template = f\"{system_prompt}\\n{user_prompt}\\n\\nSuggestion:\\n\"\n",
        "\n",
        "    # Generating the response (assuming lcpp_llm is already loaded)\n",
        "    response = lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
        "                        repeat_penalty=1.2, top_k=150,\n",
        "                        echo=False)  # Set echo to False to exclude the prompt from the response\n",
        "\n",
        "    # Display the response to the user\n",
        "    print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "KCR8dVAQhQzi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from threading import Timer\n",
        "\n",
        "# Introduction message\n",
        "print(\"Hello! I'm Quick_Tips, your digitalized encyclopedia. I can provide up-to-date knowledge about the world.\")\n",
        "print(\"What would you like to know today?\")\n",
        "\n",
        "# Creating an input box for user input\n",
        "input_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your question here...',\n",
        "    description='Your Question:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "display(input_box)\n",
        "\n",
        "def on_enter_pressed(event):\n",
        "    if event.value:\n",
        "        ask_question(event.value)\n",
        "    else:\n",
        "        print(\"Please type a question to get an answer.\")\n",
        "\n",
        "input_box.on_submit(on_enter_pressed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "c3390e435046459bae6f9808d8805898",
            "f079b44cbd004b96922948c612659a0f",
            "b329c1cb170f49878334dffc6560c85f"
          ]
        },
        "id": "1EyEMwLgbd7X",
        "outputId": "9b442f2a-93a3-4ff5-8ace-7c227c4ab315"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm Quick_Tips, your digitalized encyclopedia. I can provide up-to-date knowledge about the world.\n",
            "What would you like to know today?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Your Question:', placeholder='Type your question here...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3390e435046459bae6f9808d8805898"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You could also ask for more information, like \"What kind of compass?\" or \"For what purpose?\", in order to give a more accurate answer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A compass is a navigational tool used for determining direction. It typically consists of a magnetic needle that points towards the Earth's magnetic North Pole, and a series of markings or a dial that indicate the cardinal directions (north, south, east, west). The compass has been an essential instrument for navigation since ancient times, and is still widely used today in various fields such as geography, cartography, sailing, hiking, and aviation.\n",
            "\n",
            "Would you like to know more about the history of the compass? Or perhaps you'd prefer information on how to use a compass for navigation?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "infmOY3Ibhp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}